# Процесс миграции на типизированные события
* Т.к. версионирование было не предусмотренно структурой событий - для новых событий с поддержкой версионирования будем использовать новую очередь. Заодно поправим нейминг очередей.
  * accounts-cud => accounts-stream
  * tasks-cud => tasks-stream
  * tasks => tasks-workflow
* Для SchemaRegistry выделяем отдельный проект aTES.SchemaRegistry. Делаем типизированные DTO + json schema для валидации
* Общий формат сообщений `{id:string, version:string, type:string, created_at:string, producer:string, payload:{}}`
* Формат сериализации - по прежнему JSON
* Шаги миграции (обобщенно)
  1. Добавляем новых консьюмеров, использующих новые очереди и новый формат сообщений (version = 1)
  2. Добавляем новых продьюсеров
  3. (деплой)
  4. Убираем старые продьюсеры
  5. (деплой)
  6. Проверяем, что нет не обработанных сообщений старого формата
  7. Убираем старые консьюмеры

# ДЗ
# Общие замечания
* Для примера в рамках PR по уроку 6-7 остановимся на шаге 6 - оставим два консьюмера, но одного продьюсера
* Для экономии времени реализуем валидацию формата событий только за счет возможностей языка (типизация)
* Полноценное решение должно включать: 
  * Отдельный платформенно-независимый проект/сервис с json схемами
  * Валидацию отправляемых и принимаемых событий по схеме (для .NET: https://www.newtonsoft.com/json/help/html/JsonSchema.htm)
  * Трекинг используемых схем и прочие плюшки

## Jira-id
* Считаем, что бизнесу ок, что в старых задачах в заголовке останется Jira-id где он был добавлен руками.
* Делаем миграцию БД
  * добавляем sequence
  * обновляем старые записи, нумеруя их
* Т.к. префикс пока один (UBERPOPUG) - не храним его в БД (подставляем при чтении из БД)
* Обновляем UI - выводим ид задачи
* Миграция событий
  * Изменение обратно совместимое, достаточно расширить CUD событие новым полем и инициировать стриминг текущего состояния
  * Но данная доработка - хороший момент добавить версионирование и сменить имя очереди для стриминга. Поэтому
    * Переводим тасктрекер на продьюсинг событий с версионируемой схемой (пока только стриминг)
    * Добавляем новую версию событий (v1) с новыми полями
    * Новые сообщения отправляем в топик с другим именем (для его переименования, как писал выше)
    * Консьюмеров пока нет, поэтому начинаем с продьюсера    
   * Стримим новое поле
     * Добавляем в таск трекер новую кнопку - "отправить текущее состояние задач"
     * Кнопка доступна только суперюзеру
     * После деплоя системы нажимаем кнопку, чтобы заполнить топик начальными данными
## Смена названия статусов
* Т.к. меняется название статусов - меняется и название событий. Это обратно не совместимое изменение для бизнес событий
* Миграция БД не требуется, статус в БД не хранится (хранится булевый признак завершенности)
* Миграция событий
  * Переводим тасктрекер на продьюсинг событий с версионируемой схемой (теперь бизнес события)
    * Добавляем новую версию событий (v1) с новыми именами (заодно поддержим новое поле JiraId)
    * Новые сообщения отправляем в топик с другим именем (для его переименования, как писал выше)
    * Консьюмеров пока нет, поэтому начинаем с продьюсера 
    * Чтобы мигрировать старые события - добавляем консьюмера на старую очередь, который переложит события в новую с новым форматом. Это решение подходит, т.к. консьюмеров еще нет. Если бы были - возникла бы двойная обработка и необходимо было бы другое решение (например держать старую очередь в системе в RO режиме, чтобы новые сервисы потребляли сначала ее, а после переключались на новую).
    * Т.к. мы на косячили и в старом формате нет никакого идентификатора типа события - будем определять его по косвенным признакам (наличию полей)        
  * Стриминг событие не меняем, т.к. в нем статус передается числом
## Реализация билинга
* Для экономии времени
  * вместо интеграцию с сервисом платежей (3rd party) и email отчеты реализуем в виде заглушек с логированием в консоль
  * не обрабатываем CUD события изменения тасок (по ТЗ они нужны для фиксации актуального описания задачи в момент вставки в лог транзакций)
  * не реализуем логику обработки событий с неизвестной версией или неизвестным типом
  * выводим весь аудит лог на одной странице
* Также, для простоты отладки, будем тригерить закрытие операционного дня не по расписанию, а по нажатию кнопки попугом с ролью бухгалтера
* Для экономии времени вместо полноценной обработки платежа будем просто обновлять баланс и добавлять запись в аудит лог. Полноценная реализация выглядела бы следующей:
  * тригерим событие "выплата для пользователя Х расчитана" с количеством $ к выплате
  * подписчик выполняет взаимодействие с 3rd party сервисом платежей, по окончанию тригерит событие "выплата осуществлена"
  * подписчик на "выплата осуществлена" обновляет баланс и обновляет лог
  * другой подписчик на тоже самое событие отправляет email уведомление.
  * Вместо этого выполним только синхронное обновление балансов и логов по событию окончания операционного дня.
## Выбор стратегии обработки ошибок связанных с аккаунтингом
* Для того, чтобы гарантировать обработку всех входящих событий
  * если получили событие неизвестного типа - будем сохранять его в БД+ настроим алярм на наличие таких событий. После получения алярма разработчик-попуг должен будет реализовать поддержку этого события (либо добавить его в исключения) и реализовать разбор событий накопившихся в БД
  * если получили ошибку при обработке, сообщение не поддерживаемой версии или нарушающее контракт - поступаем аналогично (сохранение -> ручной анализ -> обработка кодом по возможности)
* Чтобы исключить двойную обработку событий - добавим отдельную таблицу с ИДами обработанных сообщений и уникальным индексом, вставлять данные в эту таблицу в той же транзакции, что и обновлять бизнес данные (т.е. если событие было уже обработано - получим ошибку сохранения)
* Для исходящих событий реализуем transactional outbox - будем сохранять события в БД и отправлять их в фоне, если брокер не доступен будем использовать экспоненциальные ретраи